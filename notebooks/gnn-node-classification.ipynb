{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Network (GCN)\n",
    "\n",
    "We demonstrated the use of GCN for node attribute inference on the CORA paper citation dataset.\n",
    "\n",
    "**References**\n",
    "\n",
    "[Semi-Supervised Classification with Graph Convolutional Networks](https://www.thejournal.club/c/paper/101516/), T. N. Kipf and M. Welling, ICLR 2017\n",
    "\n",
    "\n",
    "Copyright 2010-2021 Commonwealth Scientific and Industrial Research Organisation (CSIRO).\n",
    "\n",
    "All Rights Reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CORA network\n",
    "\n",
    "This time we are going to load the dataset from `dgl.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset = dgl.data.CoraGraphDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time you run the above command, DGL will download the dataset and store it locally.\n",
    "\n",
    "The dataset comes with train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cora_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset consists of a single graph\n",
    "len(cora_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_graph = cora_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access node and edge data via the **`ndata`** and **`edata`** member variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_graph.ndata['train_mask'], len(cora_graph.ndata['train_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node labels 1 of 7 in range [0, 6]\n",
    "cora_graph.ndata['label'], np.unique(cora_graph.ndata['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node features\n",
    "cora_graph.ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we have 2708 nodes with 1433-dimensional feature vector for each.\n",
    "cora_graph.ndata['feat'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degrees = cora_graph.in_degrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 7,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(node_degrees);\n",
    "plt.xlabel(\"Node degree\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degrees[node_degrees > 70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already split into train, validation and test sets. Let's check their sizes. The splits are from the paper [Revisiting Semi-Supervised Learning with Graph Embeddings\n",
    "](https://www.thejournal.club/c/paper/90881/) by Z. Yang et.al., ICML 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of train examples     : {cora_graph.ndata['train_mask'].sum().item()}\")\n",
    "print(f\"Number of validation examples: {cora_graph.ndata['val_mask'].sum().item()}\")\n",
    "print(f\"Number of test examples      : {cora_graph.ndata['test_mask'].sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A graph convolutional layer as defined by Kipf and Welling.\n",
    "from dgl.nn import GraphConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats: int, h_feats: list[int], num_classes: int, dropout: float=0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_feats: <int> Dimensionality of node input features\n",
    "        :param h_feats: <list> Dimensionality of hidden layers\n",
    "        :param num_classes: <int> Number of output classes\n",
    "        :param dropout: <float> The amount of dropout for all but the last\n",
    "            layer. It should be a value in [0.0, 1.0]\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(GraphConv(in_feats, h_feats[0]))\n",
    "\n",
    "        for i in range(1, len(h_feats)):\n",
    "            self.conv_layers.append(GraphConv(h_feats[i-1], h_feats[i]))\n",
    "\n",
    "        self.conv_layers.append(GraphConv(h_feats[-1], num_classes))\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "\n",
    "        h = in_feat\n",
    "\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            h = layer(g, h)\n",
    "            if i < len(self.conv_layers)-1:\n",
    "                h = F.dropout(F.relu(h), p=self.dropout)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a GCN model with 1 hidden graph convolutional layer and 1 output layer which will also be graph convolutional. The output node embeddings will be 16-dimensional as this is the number of neurons in the hidder layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model = GCN(in_feats=cora_graph.ndata['feat'].shape[1], h_feats=[16], num_classes=cora_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the model layers\n",
    "gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, model, epochs=200, lr=0.01, weight_decay=0.0005, verbose=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    losses = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    accs = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we only need the loss over the nodes in the training set for\n",
    "        # updating the model parameters but we compute it for the validation and test nodes\n",
    "        # for reporting.\n",
    "        loss = torch.nn.functional.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        losses[\"train\"].append(loss.item())\n",
    "        losses[\"val\"].append(torch.nn.functional.cross_entropy(logits[val_mask], labels[val_mask]).item())\n",
    "        losses[\"test\"].append(torch.nn.functional.cross_entropy(logits[test_mask], labels[test_mask]).item())\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        # Compute accuracy on training/validation/test\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        accs[\"train\"].append(train_acc)\n",
    "        accs[\"val\"].append(val_acc)\n",
    "        accs[\"test\"].append(test_acc)\n",
    "        \n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        \n",
    "        if verbose and e % 10 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "            \n",
    "    return losses, accs, best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_losses, gcn_accs, best_test_acc = train(cora_graph, gcn_model, epochs=500, lr=0.01, weight_decay=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gcn_losses[\"train\"], label=\"train\")\n",
    "plt.plot(gcn_losses[\"val\"], label=\"val\")\n",
    "plt.plot(gcn_losses[\"test\"], label=\"test\")\n",
    "plt.title(\"GCN Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gcn_accs[\"train\"], label=\"train\")\n",
    "plt.plot(gcn_accs[\"val\"], label=\"val\")\n",
    "plt.plot(gcn_accs[\"test\"], label=\"test\")\n",
    "plt.title(\"GCN Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a function of number of GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = 12\n",
    "gcn_models = []\n",
    "for num_layers in range(1, max_layers+1):\n",
    "    gcn_models.append(GCN(in_feats=cora_graph.ndata['feat'].shape[1], h_feats=[16]*num_layers, num_classes=cora_dataset.num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = []\n",
    "for model in gcn_models:\n",
    "    gcn_losses, gcn_accs, best_test_acc = train(cora_graph, model, epochs=500, lr=0.01, weight_decay=0.0005, verbose=False)\n",
    "    test_accs.append(best_test_acc.item()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_accs = [acc*100 for acc in test_accs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, max_layers+1), test_accs[:-1])\n",
    "plt.xlabel(\"Number of Layers\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "# plt.plot(gcn_losses[\"val\"], label=\"val\")\n",
    "# plt.plot(gcn_losses[\"test\"], label=\"test\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "#plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to visualise the node representations as output by both the first and second graph convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conv_layer = gcn_model.conv_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_1 = graph_conv_layer(cora_graph, cora_graph.ndata[\"feat\"]).detach().numpy()\n",
    "node_embeddings_2 = gcn_model(cora_graph, cora_graph.ndata[\"feat\"]).detach().numpy()\n",
    "node_embeddings_1.shape, node_embeddings_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We see that there is one 16D vector output from the first convolutional layer and a 7D vector output from the second convolutional network for each of the 2708 nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = TSNE\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_transformed_1 = trans.fit_transform(node_embeddings_1)\n",
    "\n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_transformed_2 = trans.fit_transform(node_embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_transformed_1.shape, node_embeddings_transformed_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for plotting node embeddings\n",
    "def plot_embeddings(node_embeddings, ax, node_labels, title, x_label=\"$X_1$\", y_label=\"$X_2\", alpha=0.7, figsize=(7,7)):\n",
    "    ax.scatter(node_embeddings[:, 0], \n",
    "               node_embeddings[:, 1], \n",
    "               c=node_labels, \n",
    "               cmap=\"jet\", alpha=alpha)\n",
    "    ax.set(aspect=\"equal\", xlabel=x_label, ylabel=y_label)\n",
    "    ax.set(aspect=\"equal\", xlabel=x_label, ylabel=y_label)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_1, ax_2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "plot_embeddings(node_embeddings_transformed_1, \n",
    "                ax_1,\n",
    "                cora_graph.ndata['label'], \n",
    "                title='Layer 1 embeddings', \n",
    "                x_label=\"$X_1$\", \n",
    "                y_label=\"$X_2$\", \n",
    "                alpha=0.7, \n",
    "                figsize=(8,8))\n",
    "plot_embeddings(node_embeddings_transformed_2, \n",
    "                ax_2,\n",
    "                cora_graph.ndata['label'], \n",
    "                title='Layer 2 embeddings', \n",
    "                x_label=\"$X_1$\", \n",
    "                y_label=\"$X_2$\", \n",
    "                alpha=0.7, \n",
    "                figsize=(8,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Can you improve classification performance?\n",
    "\n",
    "Consider using more layers, wider layers, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_gcn_model = GCN(in_feats=cora_graph.ndata['feat'].shape[1], \n",
    "                     h_feats=[16]*20,    # Specify 20 graph convolutional layers\n",
    "                     num_classes=cora_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_gcn_losses, deep_gcn_accs, deep_gcn_best_test_acc = train(cora_graph, deep_gcn_model, epochs=500, lr=0.01, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deep_gcn_losses[\"train\"], label=\"train\")\n",
    "plt.plot(deep_gcn_losses[\"val\"], label=\"val\")\n",
    "plt.plot(deep_gcn_losses[\"test\"], label=\"test\")\n",
    "plt.title(\"Deep GCN Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deep_gcn_accs[\"train\"], label=\"train\")\n",
    "plt.plot(deep_gcn_accs[\"val\"], label=\"val\")\n",
    "plt.plot(deep_gcn_accs[\"test\"], label=\"test\")\n",
    "plt.title(\"GCN Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_node_embeddings = deep_gcn_model(cora_graph, cora_graph.ndata[\"feat\"]).detach().numpy()\n",
    "deep_node_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transform(n_components=2)\n",
    "deep_node_embeddings_transformed = trans.fit_transform(deep_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9,9))\n",
    "plot_embeddings(deep_node_embeddings_transformed,\n",
    "                ax,\n",
    "                cora_graph.ndata['label'], \n",
    "                title='Visualization of Deep GCN embeddings for cora dataset', \n",
    "                x_label=\"$X_1$\", \n",
    "                y_label=\"$X_2$\", \n",
    "                alpha=0.7, \n",
    "                figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Graph Attention Network (GAT)\n",
    "\n",
    "We demonstrated the use of GAT for node attribute inference on the CORA paper citation dataset.\n",
    "\n",
    "**References**\n",
    "\n",
    "[Graph Attention Networks](https://www.thejournal.club/c/paper/134548/), P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio, ICLR 2018\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A graph attention convolutional layer\n",
    "from dgl.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats: int, h_feats: list[int], attention_heads: list[int], num_classes: int, \n",
    "                 feat_dropout :float=0, attention_dropout: float=0, concat_heads: bool=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_feats: <int> Dimensionality of node input features.\n",
    "        :param h_feats: <list> Dimensionality of hidden layers.\n",
    "        :param attention_heads: <list> Number of attention heads for each layer.\n",
    "        :param num_classes: <int> Number of output classes.\n",
    "        :param feat_dropout: <float> The amount of dropout for layer input.\n",
    "        :param attention_dropout: <float> The amount of dropout for attention coefficients.\n",
    "        :param concat_heads: <bool> If True attention head outputs are concatenated or averaged if False.\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.concat_heads = concat_heads\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(GATConv(in_feats, \n",
    "                                        h_feats[0], \n",
    "                                        num_heads=attention_heads[0], \n",
    "                                        feat_drop=feat_dropout, \n",
    "                                        attn_drop=attention_dropout))\n",
    "\n",
    "        for i in range(1, len(h_feats)):\n",
    "            self.conv_layers.append(GATConv(h_feats[i-1]*attention_heads[-1] if concat_heads else h_feats[i-1], \n",
    "                                            h_feats[i], \n",
    "                                            num_heads=attention_heads[i], \n",
    "                                            feat_drop=feat_dropout,\n",
    "                                            attn_drop=attention_dropout))\n",
    "\n",
    "        self.conv_layers.append(GATConv(h_feats[-1]*attention_heads[-2] if concat_heads else h_feats[-1], \n",
    "                                        num_classes, \n",
    "                                        num_heads=attention_heads[-1],\n",
    "                                        feat_drop=feat_dropout,\n",
    "                                        attn_drop=attention_dropout))\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "\n",
    "        h = in_feat\n",
    "\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            h = layer(g, h)  # output tensor is N, H, D_out where H is number of heads\n",
    "            if self.concat_heads:\n",
    "                h = h.reshape(h.shape[0], -1)\n",
    "            else:\n",
    "                # We are just going to average the node embeddings across attention heads\n",
    "                h = h.mean(axis=-2)\n",
    "                \n",
    "            if i < len(self.conv_layers)-1:\n",
    "                h = F.elu(h)                \n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model = GAT(in_feats=cora_graph.ndata['feat'].shape[1], \n",
    "                h_feats=[8], # Dimensionality of node embeddings for each attention head\n",
    "                attention_heads=[8,1], # Number of attention heads for each graph convolutional layer\n",
    "                num_classes=cora_dataset.num_classes,\n",
    "                feat_dropout=0.6,\n",
    "                attention_dropout=0.6,\n",
    "                concat_heads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_losses, gat_accs, gat_best_acc = train(cora_graph, gat_model, epochs=500, lr=0.005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gat_losses[\"train\"], label=\"train\")\n",
    "plt.plot(gat_losses[\"val\"], label=\"val\")\n",
    "plt.plot(gat_losses[\"test\"], label=\"test\")\n",
    "plt.title(\"GAT Loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gat_accs[\"train\"], label=\"train\")\n",
    "plt.plot(gat_accs[\"val\"], label=\"val\")\n",
    "plt.plot(gat_accs[\"test\"], label=\"test\")\n",
    "plt.title(\"GAT Accuracy\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "\\[1\\] How does GAT performance change as a function of the GNNs depth? Does a GNN with GAT convolutionl layers suffer from the same oversmoothing problem as a deep GNN with GCN layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GCN2Conv\n",
    "from torch.nn import Linear\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNII(nn.Module):\n",
    "    def __init__(self, in_feats: int, h_feats: list[int], num_classes: int, dropout: float=0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_feats: <int> Dimensionality of node input features\n",
    "        :param h_feats: <list> Dimensionality of hidden layers; all hidden layers must have\n",
    "            the same number of hidden units.\n",
    "        :param num_classes: <int> Number of output classes\n",
    "        :param dropout: <float> The amount of dropout for all but the last\n",
    "            layer. It should be a value in [0.0, 1.0]\n",
    "        \"\"\"\n",
    "        super(GCNII, self).__init__()\n",
    "\n",
    "        if len(set(h_feats)) != 1:\n",
    "            raise ValueError(f\"All hidden layers must have the same number of hidden units but given {h_feats}\")\n",
    "        \n",
    "        self.dropout = dropout        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.linear = Linear(in_feats, h_feats[0])\n",
    "        \n",
    "        self.conv_layers.append(GCN2Conv(h_feats[0], h_feats[0]))\n",
    "\n",
    "        for i in range(1, len(h_feats)):\n",
    "            self.conv_layers.append(GCN2Conv(h_feats[i-1], h_feats[i]))\n",
    "\n",
    "        self.conv_layers.append(GCN2Conv(h_feats[-1], num_classes))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.linear.reset_parameters()\n",
    "        for layer in self.conv_layers:\n",
    "            std = 1.0 / math.sqrt(layer._in_feats)\n",
    "            torch.nn.init.uniform_(layer.weight1, -std, std)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "\n",
    "        h_0 = self.linear(F.dropout(in_feat, p=self.dropout))\n",
    "        h = h_0\n",
    "        \n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            h = layer(g, F.dropout(h, p=self.dropout), h_0)\n",
    "            if i < len(self.conv_layers)-1:\n",
    "                h = F.relu(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn2_model = GCNII(in_feats=cora_graph.ndata['feat'].shape[1], \n",
    "                   h_feats=[16]*2,    # Specify 2 graph convolutional layers\n",
    "                   dropout=0.2,\n",
    "                   num_classes=cora_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn2_model.reset_parameters()\n",
    "gcn2_losses, gcn2_accs, best_gcn2_test_acc = train(cora_graph, gcn2_model, epochs=500, lr=0.01, weight_decay=0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn2_model = GCNII(in_feats=cora_graph.ndata['feat'].shape[1], \n",
    "                   h_feats=[16]*32,    # Specify 32 graph convolutional layers\n",
    "                   dropout=0.2,\n",
    "                   num_classes=cora_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn2_model.reset_parameters()\n",
    "gcn2_losses, gcn2_accs, best_gcn2_test_acc = train(cora_graph, gcn2_model, epochs=500, lr=0.01, weight_decay=0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grl-course-env",
   "language": "python",
   "name": "grl-course-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
