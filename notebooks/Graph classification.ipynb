{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ef87b1",
   "metadata": {},
   "source": [
    "# GNNs for Graph Classification\n",
    "\n",
    "Example of using GNNs for graph classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cda056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "\n",
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "from dgl.data.utils import split_dataset\n",
    "from dgl.dataloading.pytorch import GraphDataLoader\n",
    "from dgl.nn import GraphConv, SumPooling, AvgPooling, MaxPooling, GlobalAttentionPooling\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8848165",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to use the `ENZYMES` dataset. There are 600 graphs in this dataset and each graph corresponds to a molecule. Our goal is to train a model to predict one of 6 properties for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(name='enzymes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e447690",
   "metadata": {},
   "source": [
    "The dataset comprises of 600 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95632c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48775cc2",
   "metadata": {},
   "source": [
    "Let's have a look at the first graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b405c72",
   "metadata": {},
   "source": [
    "Each element of `dataset` is a 2-tuple where the first element is the graph object and the second the graph's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b2a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8bda2",
   "metadata": {},
   "source": [
    "Let's visualize the first graph. We are going to use `NetworkX` for graph drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09364c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_nx = dgl.to_networkx(dataset[0][0])\n",
    "nx.draw(graph_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c0858",
   "metadata": {},
   "source": [
    "The nodes in the graph have 18-dimensional feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c03e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].ndata[\"node_attr\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f18975",
   "metadata": {},
   "source": [
    "The graph labels are integers in the range \\[0,5\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ede05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a436a0a",
   "metadata": {},
   "source": [
    "We are going to add self loops to all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66592d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "        (dgl.add_self_loop(dgl.remove_self_loop(el[0])), el[1]) for el in dataset\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde43c41",
   "metadata": {},
   "source": [
    "Let's split our data into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_dataset(dataset, \n",
    "                                                frac_list=[0.8, 0.1, 0.1], \n",
    "                                                shuffle=True, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd6a4d",
   "metadata": {},
   "source": [
    "For model training and evaluation, we need to create data loaders. We do this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = GraphDataLoader(train_data, batch_size=32, shuffle=True, drop_last=False, num_workers=0)\n",
    "val_data_loader = GraphDataLoader(val_data, batch_size=32, shuffle=True, drop_last=False, num_workers=0)\n",
    "test_data_loader = GraphDataLoader(test_data, batch_size=32, shuffle=True, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b2fa4",
   "metadata": {},
   "source": [
    "## The GNN Model\n",
    "\n",
    "The model we are considering will consist of 2 GCN layers followed by a pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5476898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, num_classes, activation, pooling=\"mean\"):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcn_1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.gcn_2 = GraphConv(hidden_feats, num_classes)\n",
    "        \n",
    "        self.pool = None\n",
    "        if pooling == \"mean\":\n",
    "            self.pool = AvgPooling()\n",
    "        elif pooling == \"sum\":\n",
    "            self.pool = SumPooling()\n",
    "        elif pooling == \"max\":\n",
    "            self.pool = MaxPooling()\n",
    "        elif pooling == \"attention\":            \n",
    "            self.pool = GlobalAttentionPooling(torch.nn.Linear(num_classes, 1))\n",
    "        else:\n",
    "            raise Exception(f\"pooling should be one of mean, sum, max, attention but found {pooling}\")\n",
    "            \n",
    "        self.activation = activation()\n",
    "    \n",
    "    def forward(self, graph, features):\n",
    "        h = self.gcn_1(graph, features)\n",
    "        h = self.activation(h)\n",
    "        h = self.gcn_2(graph, h)\n",
    "        h = self.pool(graph, h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.gcn_1.reset_parameters()\n",
    "        self.gcn_2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(in_feats=18, hidden_feats=16, num_classes=6, activation=torch.nn.ReLU, pooling=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204087d",
   "metadata": {},
   "source": [
    "Setup the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25890981",
   "metadata": {},
   "source": [
    "Lastly, we need to specify what loss to use. Since we are solving a multi-class classification problem, we are going to optimise the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e68a9d",
   "metadata": {},
   "source": [
    "Let us create two helper methods, one for training our model and one for calculating the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, criterion, train_data_loader, val_data_loader, test_data_loader, epochs=100):\n",
    "    \n",
    "    losses_and_metrics = { \"train_loss\": [], \"train_acc\": [],\n",
    "                           \"val_loss\": [], \"val_acc\": [],\n",
    "                           \"test_loss\": [], \"test_acc\": [] }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for graphs, labels in train_data_loader:  # one batch of graphs\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            node_features = graphs.ndata[\"node_attr\"].float()\n",
    "            pred = model(graphs, node_features)\n",
    "            \n",
    "            loss = criterion(pred, labels.squeeze(-1))\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # evaluate the model on the three datasets\n",
    "        for eval_data_loader, dataset_name in zip([train_data_loader, val_data_loader, test_data_loader], [\"train\", \"val\", \"test\"]):\n",
    "            acc, loss =predict(model, criterion, eval_data_loader)        \n",
    "            losses_and_metrics[f\"{dataset_name}_loss\"].append(loss)\n",
    "            losses_and_metrics[f\"{dataset_name}_acc\"].append(acc)\n",
    "            \n",
    "    return losses_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    y_hat = []\n",
    "    y = []\n",
    "    loss = 0\n",
    "    num_graphs = 0\n",
    "    for graphs, labels in data_loader:\n",
    "        node_features = graphs.ndata[\"node_attr\"].float()\n",
    "        labels = labels.squeeze(-1)\n",
    "        pred = model(graphs, node_features)\n",
    "        loss += criterion(pred, labels).item()*len(labels)\n",
    "        num_graphs += len(labels)\n",
    "\n",
    "        y_hat.extend(pred.argmax(1))\n",
    "        y.extend(labels)\n",
    "            \n",
    "    return accuracy_score(y, y_hat), loss / num_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97540dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_parameters()\n",
    "losses_and_metrics = train(model, optimiser, criterion, train_data_loader, val_data_loader, test_data_loader, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea4cc1",
   "metadata": {},
   "source": [
    "Let's plot the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29517e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_and_metrics[\"train_loss\"], label=\"train\")\n",
    "plt.plot(losses_and_metrics[\"val_loss\"], label=\"val\")\n",
    "plt.plot(losses_and_metrics[\"test_loss\"], label=\"test\")\n",
    "plt.title('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_and_metrics[\"train_acc\"], label=\"train\")\n",
    "plt.plot(losses_and_metrics[\"val_acc\"], label=\"val\")\n",
    "plt.plot(losses_and_metrics[\"test_acc\"], label=\"test\")\n",
    "plt.title('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea5608",
   "metadata": {},
   "source": [
    "## Jumping Knowledge Network\n",
    "\n",
    "We are going to extend our graph classification GNN with jump connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JKNet(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, num_classes, activation, pooling=\"mean\"):\n",
    "        super(JKNet, self).__init__()\n",
    "        self.gcn_1 = GraphConv(in_feats, hidden_feats)\n",
    "        self.gcn_2 = GraphConv(hidden_feats, hidden_feats)\n",
    "        self.linear = torch.nn.Linear(hidden_feats*2, num_classes)\n",
    "        \n",
    "        self.pool = None\n",
    "        if pooling == \"mean\":\n",
    "            self.pool = AvgPooling()\n",
    "        elif pooling == \"sum\":\n",
    "            self.pool = SumPooling()\n",
    "        elif pooling == \"max\":\n",
    "            self.pool = MaxPooling()\n",
    "        elif pooling == \"attention\":\n",
    "            self.pool = GlobalAttentionPooling(torch.nn.Linear(hidden_feats, 1))\n",
    "        else:\n",
    "            raise Exception(f\"pooling should be one of mean, sum, max, attention but found {pooling}\")\n",
    "            \n",
    "        self.activation = activation()\n",
    "    \n",
    "    def forward(self, graph, features):\n",
    "        h = self.activation(self.gcn_1(graph, features))\n",
    "        h_1 = self.pool(graph, h)\n",
    "        h = self.activation(self.gcn_2(graph, h))\n",
    "        h_2 = self.pool(graph, h)\n",
    "        h = torch.cat([h_1, h_2], dim=1)\n",
    "        h = self.linear(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.gcn_1.reset_parameters()\n",
    "        self.gcn_2.reset_parameters()\n",
    "        self.linear.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jknet = JKNet(in_feats=18, hidden_feats=16, num_classes=6, activation=torch.nn.ReLU, pooling=\"max\")\n",
    "optimiser = torch.optim.Adam(model_jknet.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jknet.reset_parameters()\n",
    "losses_and_metrics = train(model_jknet, optimiser, criterion, train_data_loader, val_data_loader, test_data_loader, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_and_metrics[\"train_loss\"], label=\"train\")\n",
    "plt.plot(losses_and_metrics[\"val_loss\"], label=\"val\")\n",
    "plt.plot(losses_and_metrics[\"test_loss\"], label=\"test\")\n",
    "plt.title('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_and_metrics[\"train_acc\"], label=\"train\")\n",
    "plt.plot(losses_and_metrics[\"val_acc\"], label=\"val\")\n",
    "plt.plot(losses_and_metrics[\"test_acc\"], label=\"test\")\n",
    "plt.title('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bf1fe",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Can you improve the results? What if you use a deeper or wider or both GNN?\n",
    "2. Replace GCNConv with GATConv. Does it improve performance?\n",
    "3. For JKNet, conder concatenating the node features before graph pooling. Does it make a difference?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRL Course",
   "language": "python",
   "name": "grl-course-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
